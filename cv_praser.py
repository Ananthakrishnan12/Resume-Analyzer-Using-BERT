# -*- coding: utf-8 -*-
"""CV_praser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kn8AVtVXduiLidRZ-fyASDacRpgdQqSm
"""

#!pip install transformers torch spacy fitz PyMuPDF fastapi uvicorn sentence-transformers tools python-multipart

#!python -m spacy download en_core_web_sm

import pymupdf
import spacy

# Load SpaCy model
nlp = spacy.load("en_core_web_sm")

def extract_text_from_pdf(pdf_path):
    """Extracts text from a PDF file"""
    doc = pymupdf.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# Test with a sample PDF
pdf_path = r"/content/drive/MyDrive/Resumes/Resumes 2025/"
resume_text = extract_text_from_pdf(pdf_path)
print(resume_text)

import re

def extract_experience(text):
    """Extracts years of experience using regex"""
    exp_pattern = r'(\d+)\s*(?:years|yrs|year|yr)\s*(?:of)?\s*experience'
    matches = re.findall(exp_pattern, text, re.IGNORECASE)
    return max(map(int, matches), default=0)  # Return max years

def extract_education(text):
    """Extracts education degrees using regex"""
    degrees = ["B\.?Tech", "M\.?Tech", "B\.?Sc", "M\.?Sc", "MBA", "PhD"]
    pattern = r'\b(?:' + '|'.join(degrees) + r')\b'
    matches = re.findall(pattern, text, re.IGNORECASE)
    return list(set(matches))  # Unique matches

experience = extract_experience(resume_text)
education = extract_education(resume_text)

print("Extracted Experience:", experience, "years")
print("Extracted Education:", education)

from transformers import AutoModel, AutoTokenizer
import torch

# Load BERT model & tokenizer
bert_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

def extract_skills(text):
    """Extracts skills from a resume using predefined skill set"""
    predefined_skills = ["Python", "Machine Learning", "Deep Learning", "NLP", "Django", "FastAPI"]
    extracted_skills = [skill for skill in predefined_skills if skill.lower() in text.lower()]
    return extracted_skills

skills = extract_skills(resume_text)
print("Extracted Skills:", skills)

def encode_text(text):
    """Generates BERT embeddings for text"""
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = bert_model(**inputs)
    return outputs.last_hidden_state.mean(dim=1)  # Average pooling

# Generate embeddings for resume & job descriptions
resume_embedding = encode_text(resume_text)
job_description = "Looking for a Python Developer with experience in Python, Django, FastAPI."
job_embedding = encode_text(job_description)

print("Resume embedding shape:", resume_embedding.shape)
print("Job description embedding shape:", job_embedding.shape)

from torch.nn.functional import cosine_similarity

def match_resume_with_job(resume_emb, job_emb):
    """Computes cosine similarity between resume & job"""
    similarity = cosine_similarity(resume_emb, job_emb)
    return similarity.item()

similarity_score = match_resume_with_job(resume_embedding, job_embedding)
print("Job Matching Score:", similarity_score)